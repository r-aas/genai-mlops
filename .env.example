# ─────────────────────────────────────────────────
# GenAI MLOps Stack — Environment Variables
# Copy to .env and customize. Secrets are in secrets/ (run: task init-secrets)
# ─────────────────────────────────────────────────

# ── n8n ──────────────────────────────────────────
N8N_VERSION=1.123.21
N8N_PORT=5678
TIMEZONE=America/Denver

# ── n8n Postgres ─────────────────────────────────
N8N_POSTGRES_VERSION=16.8
N8N_POSTGRES_USER=n8n
N8N_POSTGRES_DB=n8n

# ── MLflow ───────────────────────────────────────
MLFLOW_VERSION=v3.10.0
# NOTE: macOS AirPlay Receiver squats on port 5000. Use 5050 to avoid conflict.
MLFLOW_PORT=5050

# ── MLflow Postgres ──────────────────────────────
MLFLOW_POSTGRES_VERSION=16.8
MLFLOW_POSTGRES_USER=mlflow
MLFLOW_POSTGRES_DB=mlflow

# ── MinIO (S3-compatible artifact store) ─────────
# Pinned: last publicly available Docker Hub releases (repo archived Feb 2026)
MINIO_VERSION=RELEASE.2025-04-22T22-12-26Z
MINIO_MC_VERSION=RELEASE.2025-05-21T01-59-54Z
MINIO_ROOT_USER=minio
MINIO_PORT=9000
MINIO_BUCKET=mlflow
AWS_DEFAULT_REGION=us-east-1

# ── Inference Provider ─────────────────────────
# Provider-agnostic: swap base URL to use any OpenAI-compatible API.
# Ollama runs on the host; containers reach it via host.docker.internal.
INFERENCE_BASE_URL=http://host.docker.internal:11434/v1
INFERENCE_DEFAULT_MODEL=qwen2.5:14b
INFERENCE_ALLOWED_MODELS=qwen2.5:14b,qwen2.5:7b,qwen3:32b,qwen3:30b,mistral:7b-instruct,llama3.2:latest,nomic-embed-text:latest

# ── OpenAI SDK (host-side scripts) ─────────────
OPENAI_BASE_URL=http://localhost:11434/v1
OPENAI_API_KEY=ollama
OPENAI_MODEL=qwen2.5:14b
